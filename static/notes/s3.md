# S3

"Infinitely scaling" storage

* backup and recovery
* archiving
* application hosting
* data analytics (data lake)
* software delivery
* static sites
* durability of 11 9's: if you store 10,000,000 objects, you may lose up to appr. one object every 10,000y
* availability: depends on storage class, but standard S3 has 3 9's, equating to 53 mins/y downtime

Buckets: store objects in buckets (directories)

* buckets are per region at creation time, despite looking global
* buckets must have globally unique names
* naming conventions: no uppercase or underscores, not an IP, 3-63 char
  * cannot start with "xn--"
  * cannot end with "-s3alias"

Objects:

* objects have a full path
* the key is the full path
  * prefix (directory)
  * object name
* directory sep is purely logical '/' not real
* max object size is 5TB
  * use multi-part upload for big uploads >100MB
* metadata fields allow attaching key-value pairs to objects
* version ID included (if versioning enabled)
* pre-signed urls: these are long urls with encoded credentials to access objects.

### Security

Bucket policy:

* json based policies
  * Resources: buckets or objects
  * Effect: ALLOW / DENY
  * Actions: set of actions to allow or deny
  * Principal: the account or user to apply the policy to
* user-based: IAM policies: which API calls should be allowed for a specific IAM user
* usecases:
  * grant public acces to the bucket
  * force objects to be encrypted at upload
  * grant access to another account or service/instance role
* use example policies or Policy Generator to define policies

Note: explicit-DENY will take precedent over a bucket policy.

Resource-based:

* bucket policies: bucket wide rules from the S3 console, allows cross-account
* Object ACL: finer grain access policies (can be disabled)
* Bucket ACL: less common (can be disabled)

NOTE: an IAM user can access an S3 object if

### Static hosting

Sites are hosted via the URL http://{bucket name}.s3-website.{region}.amazonaws.com

* 403 Forbidden errors indicate you need to define a policy to enable reads

### Versioning

Enabled at the bucket level.

* same key overwrite creates a new version
* protect against unintended deletion, easily rollback versions
* any object not versioned prior to enabling it will have version = null
* deleting versioned objects is destructive/permanent
* alternatively, delete by adding a 'delete marker' to retain old items

### Replication

* CRR: cross-region replication
  * compliance, regional latency, replication across accounts
* SRR: same region replication
  * log aggregation, live replication between production/test accounts
Asynchronous copy; IAM permissions must be granted to S3 to enable replication.

NOTE: only new objects are replicated; optionally you can batch-replicate old objects.

* can replicate delete markers; in this case, permanently deleting from source bucket will not propagate to dest
* deletions with a version ID are not replicated
* there is no chaining, ie cannot replicate from bucket A to C via bucket B

Structured "replication rules" allow more complex actions, such as encryption on replication, or archiving to different storage classes.

### Storage Classes

Standard general-purpose, standard infrequent-access, one-zone infrequent access, glacier instant retrieval, glacier flexible retrieval, glacier deep archive, intelligent tiering.

* move objects manually or using lifecycle configuration

General purpose:

* 99.99% availability
* used for frequently-access data
* low latency high throughput
* sustain 2 concurrent facility failures
* usecases: big data analytics, mobile/gaming apps, content distribution

Infrequent-access (standard):

* less frequently access, but required rapid access when needed
* lower cost than standard GP
* usecases: disaster recovery, backups
* 99.9% availability

Infrequent-access (one-zone):

* high durability (99.99999999%) in a single AZ; data lost when AZ is destroyed
* 99.5% availability
* usecases: secondary backup copies, or data that you can re-create

Glacier:

* low-cost object storage for archiving/backup
* minimum storage duration of 90 days
* instant retrieval: ms retrieval, good for data accessed quarterly (business cycle oriented)
* flexible retrieval: multiple options

Intelligent-tiering:

* small monthly monitoring and auto-tiering fee
* moves objects automatically between access tiers based on usage
* no retrieval charges
* tiers:
  * frequent: default tier
  * infrequent: objects not accessed for 30 days
  * archive instant access: objects not accessed for 90 days
  * archive access: configurable from 90 days to 700 days
  * deep archive: longer

[chart](s3_comparison.png)

TODO: more in depth info on Glacier deep archive retrieval options and constraints. May need to just memorize a chart of them all.

### Moving data between storage classes

* for infrequently accessed objects, move them to Standard IA
* for archive objects to which you don't need fast access, move them to Glacier or Glacier Deep Archive

Lifecycle Rules: automate transitions between storage classes.

* Transition actions: configure objects to transition, primarily based on time.
* Expiration actions: configure objects to expire (delete) after some time
  * access log files can be set to delete after 365 days
  * can be used to delete old versions of files
  * incomplete multi-part uploads, etc
* business logic:
  * rules are defined at the prefix level
  * rules can be defined for certain object tags
* usecases: often times there are compliance rules about having a minimum time for availability of certain log files, after which they need only be rarely available.
* Analytics: these provide insights into object access frequency, providing recommendations for Standard and Standard IA (does not work for one-zone of Glacier)
  * report is updated daily with access analytics

### Event notifications

React to S3 events: ObjectCreation, ObjectRemoved, ObjectRestore, Replication, etc.

* object name filtering possible
* usecase: respond to object creation or deletion via SQS queues and SNS topics
* IAM permissions required: these must be configured on the respective receiving services, as * Resource Policy, since S3 is the sender/actor
* EventBridge: all events end up here
  * advanced filtering options
  * multiple destinations
  * capabilities: archive, replay, etc

### Performance

S3 automatically scales to high request rates, latency of 100-200ms.

* at least 3500 PUT/COPY/POST/DELETE or 5500 GET/HEAD request per second, per prefix in a bucket
  * **prefix** is the unit of performance; ergo, distribute prefixes to maximize performance
* no limit on the number of prefixes per bucket
* use multi-part upload to parallelize uploads

S3 transfer acceleration: increase transfer speed by transferring files to an AWS edge location, which will forward the data to the bucket in the target region.

* compatible with multi-part upload

Byte-range fetches: parallelize GETs by requesting specific byte ranges, used to speed up downloads.

* can be used to retrieve partial data

### Select and Glacier Select

Use a SQL like language to filter data server-side.

* filter by rows/columns
* less network transfer of unused data, less CPU client-side

### Metadata and tags

User-defined object metadata, as kvps.

* begin with 'x-amz-meta-' in lowercase
* content-length and content-type provided by default

User-defined object tags: kvps for objects.

* useful for fine-grained permissions
* useful for analytics purposes, queries

NOTE: you cannot search the metadata or tags. Instead, use an external db as a search index, such as DynamoDB.
